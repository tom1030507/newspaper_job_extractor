"""
å ±ç´™å·¥ä½œå»£å‘Šå€å¡Šæå–ç³»çµ± - æ¨¡çµ„åŒ–é‡æ§‹ç‰ˆæœ¬
ä¸»æ‡‰ç”¨ç¨‹å¼æª”æ¡ˆ
"""
import os
import threading
import time
import schedule
from datetime import datetime
from flask import Flask
from flask_socketio import SocketIO, emit

# å°å…¥é…ç½®
from config import Config, config

# å°å…¥æ¨¡å‹å’Œå­˜å„²
from models import image_storage, job_storage, progress_storage

# å°å…¥æœå‹™
from services import progress_tracker, ai_service, image_processing_service, cleanup_service

# å°å…¥è·¯ç”±
from routes import main_bp, upload_bp, results_bp

# å°å…¥å·¥å…·å‡½æ•¸
from utils import cleanup_old_files, get_storage_info

def create_app(config_name='default'):
    """æ‡‰ç”¨ç¨‹å¼å·¥å» å‡½æ•¸"""
    app = Flask(__name__, static_folder='.', static_url_path='')
    
    # è¼‰å…¥é…ç½®
    app.config.from_object(config[config_name])
    config[config_name].init_app(app)
    
    # åˆå§‹åŒ– SocketIO - æ·»åŠ æ›´å¤šé…ç½®ä»¥æ”¹å–„ Docker ç’°å¢ƒä¸‹çš„ç©©å®šæ€§
    socketio = SocketIO(
        app, 
        cors_allowed_origins=Config.CORS_ALLOWED_ORIGINS,
        async_mode='threading',  # æ˜ç¢ºæŒ‡å®šç•°æ­¥æ¨¡å¼
        logger=app.config.get('DEBUG', False),  # åœ¨èª¿è©¦æ¨¡å¼ä¸‹å•Ÿç”¨æ—¥èªŒ
        engineio_logger=app.config.get('DEBUG', False),
        ping_timeout=60,  # å¢åŠ è¶…æ™‚æ™‚é–“
        ping_interval=25,  # è¨­ç½®å¿ƒè·³é–“éš”
        allow_upgrades=True,  # å…è¨±å”è­°å‡ç´š
        transports=['websocket', 'polling']  # å…è¨±å¤šç¨®å‚³è¼¸æ–¹å¼
    )
    
    # è¨­ç½®é€²åº¦è¿½è¹¤å™¨çš„ SocketIO å¯¦ä¾‹
    progress_tracker.socketio = socketio
    
    # è¨»å†Šè—åœ–
    app.register_blueprint(main_bp)
    app.register_blueprint(upload_bp)
    app.register_blueprint(results_bp)
    
    # SocketIO äº‹ä»¶è™•ç†
    @socketio.on('connect')
    def handle_connect():
        """è™•ç†å®¢æˆ¶ç«¯é€£æ¥"""
        from flask import request
        print(f"å®¢æˆ¶ç«¯å·²é€£æ¥: {request.sid}")

    @socketio.on('disconnect')
    def handle_disconnect():
        """è™•ç†å®¢æˆ¶ç«¯æ–·é–‹é€£æ¥"""
        from flask import request
        print(f"å®¢æˆ¶ç«¯å·²æ–·é–‹: {request.sid}")

    @socketio.on('join_process')
    def handle_join_process(data):
        """å®¢æˆ¶ç«¯åŠ å…¥ç‰¹å®šçš„ process_id æˆ¿é–“"""
        from flask_socketio import join_room
        from flask import request
        process_id = data.get('process_id')
        if process_id:
            join_room(f"process_{process_id}")
            print(f"å®¢æˆ¶ç«¯ {request.sid} åŠ å…¥æˆ¿é–“: process_{process_id}")

    @socketio.on('leave_process')
    def handle_leave_process(data):
        """å®¢æˆ¶ç«¯é›¢é–‹ç‰¹å®šçš„ process_id æˆ¿é–“"""
        from flask_socketio import leave_room
        from flask import request
        process_id = data.get('process_id')
        if process_id:
            leave_room(f"process_{process_id}")
            print(f"å®¢æˆ¶ç«¯ {request.sid} é›¢é–‹æˆ¿é–“: process_{process_id}")

    @socketio.on('get_progress')
    def handle_get_progress(data):
        """è™•ç†å®¢æˆ¶ç«¯è«‹æ±‚é€²åº¦è³‡è¨Š"""
        process_id = data.get('process_id')
        progress_data = progress_tracker.get_progress(process_id)
        if progress_data:
            emit('progress_update', {
                'process_id': process_id,
                **progress_data
            })
    
    # å¥åº·æª¢æŸ¥è·¯ç”±
    @app.route('/health')
    def health_check():
        """å¥åº·æª¢æŸ¥ç«¯é»"""
        from flask import jsonify
        return jsonify({
            'status': 'healthy',
            'timestamp': datetime.now().isoformat(),
            'socketio_active': True
        }), 200

    @app.route('/')
    def root():
        """æ ¹è·¯ç”±å¥åº·æª¢æŸ¥"""
        from flask import jsonify
        return jsonify({
            'service': 'å ±ç´™å·¥ä½œå»£å‘Šæå–ç³»çµ±',
            'status': 'running',
            'timestamp': datetime.now().isoformat()
        }), 200
    
    # ç®¡ç†è·¯ç”±
    @app.route('/admin/storage')
    def admin_storage():
        """ç®¡ç†å“¡æŸ¥çœ‹å­˜å„²ç‹€æ…‹"""
        from flask import jsonify
        
        storage_info = get_storage_info(Config.RESULTS_FOLDER)
        storage_info['memory_processes'] = len(image_storage._storage)
        
        # ç²å–æœ€è¿‘çš„è™•ç†è¨˜éŒ„
        recent_processes = []
        if os.path.exists(Config.RESULTS_FOLDER):
            for item in os.listdir(Config.RESULTS_FOLDER):
                item_path = os.path.join(Config.RESULTS_FOLDER, item)
                if os.path.isdir(item_path):
                    recent_processes.append({
                        'process_id': item,
                        'created_time': datetime.fromtimestamp(os.path.getctime(item_path)).isoformat(),
                        'size_mb': round(sum(os.path.getsize(os.path.join(root, file)) 
                                           for root, dirs, files in os.walk(item_path) 
                                           for file in files) / (1024 * 1024), 2)
                    })
        
        # æŒ‰å‰µå»ºæ™‚é–“æ’åº
        recent_processes.sort(key=lambda x: x['created_time'], reverse=True)
        
        return jsonify({
            'storage_info': storage_info,
            'recent_processes': recent_processes[:10],  # åªé¡¯ç¤ºæœ€è¿‘10å€‹
            'status': 'success'
        })

    @app.route('/admin/cleanup', methods=['POST'])
    def admin_cleanup():
        """æ‰‹å‹•æ¸…ç†èˆŠæª”æ¡ˆ"""
        from flask import request, jsonify
        
        try:
            max_age_hours = request.json.get('max_age_hours', Config.CLEANUP_MAX_AGE_HOURS) if request.json else Config.CLEANUP_MAX_AGE_HOURS
            cleanup_old_files(Config.UPLOAD_FOLDER, Config.RESULTS_FOLDER, max_age_hours)
            
            # ç²å–æ¸…ç†å¾Œçš„å­˜å„²è³‡è¨Š
            storage_info = get_storage_info(Config.RESULTS_FOLDER)
            storage_info['memory_processes'] = len(image_storage._storage)
            
            return jsonify({
                'message': f'å·²æ¸…ç†è¶…é {max_age_hours} å°æ™‚çš„æª”æ¡ˆ',
                'storage_info': storage_info,
                'status': 'success'
            })
        except Exception as e:
            return jsonify({
                'error': f'æ¸…ç†å¤±æ•—: {str(e)}',
                'status': 'error'
            }), 500

    @app.route('/admin/cleanup/settings')
    def admin_cleanup_settings():
        """æŸ¥çœ‹æ¸…ç†è¨­å®šç‹€æ…‹"""
        from flask import jsonify
        
        return jsonify({
            'cleanup_settings': {
                'max_age_hours': Config.CLEANUP_MAX_AGE_HOURS,
                'interval_hours': Config.CLEANUP_INTERVAL_HOURS,
                'max_file_count': Config.CLEANUP_MAX_FILE_COUNT,
                'enable_count_limit': Config.CLEANUP_ENABLE_COUNT_LIMIT,
            },
            'current_status': {
                'results_folder': Config.RESULTS_FOLDER,
                'storage_info': get_storage_info(Config.RESULTS_FOLDER),
                'memory_processes': len(image_storage._storage),
            },
            'status': 'success'
        })

    @app.route('/admin/cleanup/count', methods=['POST'])
    def admin_cleanup_by_count():
        """æ‰‹å‹•åŸ·è¡Œæª”æ¡ˆæ•¸é‡é™åˆ¶æ¸…ç†"""
        from flask import request, jsonify
        
        try:
            max_count = request.json.get('max_count', Config.CLEANUP_MAX_FILE_COUNT) if request.json else Config.CLEANUP_MAX_FILE_COUNT
            
            # åŸ·è¡Œæª”æ¡ˆæ•¸é‡æ¸…ç†
            removed_process_ids = cleanup_service.cleanup_by_file_count(max_count)
            
            # ç²å–æ¸…ç†å¾Œçš„å­˜å„²è³‡è¨Š
            storage_info = get_storage_info(Config.RESULTS_FOLDER)
            storage_info['memory_processes'] = len(image_storage._storage)
            
            return jsonify({
                'message': f'å·²åŸ·è¡Œæª”æ¡ˆæ•¸é‡é™åˆ¶æ¸…ç†ï¼Œæœ€å¤šä¿ç•™ {max_count} å€‹æª”æ¡ˆ',
                'removed_count': len(removed_process_ids),
                'removed_process_ids': removed_process_ids,
                'storage_info': storage_info,
                'enabled': Config.CLEANUP_ENABLE_COUNT_LIMIT,
                'status': 'success'
            })
        except Exception as e:
            return jsonify({
                'error': f'æª”æ¡ˆæ•¸é‡æ¸…ç†å¤±æ•—: {str(e)}',
                'status': 'error'
            }), 500

    @app.route('/admin/cleanup/auto', methods=['POST'])
    def toggle_auto_cleanup():
        """åˆ‡æ›è‡ªå‹•æ¸…ç†åŠŸèƒ½"""
        from flask import request, jsonify
        
        try:
            enabled = request.json.get('enabled', True) if request.json else True
            
            if enabled:
                start_cleanup_scheduler()
                message = f"è‡ªå‹•æ¸…ç†å·²å•Ÿç”¨ï¼Œæ¯{Config.CLEANUP_INTERVAL_HOURS}å°æ™‚åŸ·è¡Œ"
            else:
                schedule.clear()
                message = "è‡ªå‹•æ¸…ç†å·²åœç”¨"
            
            return jsonify({
                'message': message,
                'auto_cleanup_enabled': enabled,
                'status': 'success'
            })
        except Exception as e:
            return jsonify({
                'error': f'åˆ‡æ›è‡ªå‹•æ¸…ç†å¤±æ•—: {str(e)}',
                'status': 'error'
            }), 500
    
    # Google Sheets æ•´åˆè·¯ç”±
    @app.route('/send_to_spreadsheet/<process_id>', methods=['POST'])
    def send_to_spreadsheet(process_id):
        """å°‡è™•ç†çµæœç™¼é€åˆ° Google Sheets"""
        from flask import request, jsonify
        from utils.file_utils import is_valid_job
        import requests
        
        try:
            # å¾è«‹æ±‚ä¸­ç²å– Google Apps Script URLï¼ˆç¾åœ¨è®Šç‚ºå¯é¸ï¼‰
            data = request.get_json() if request.get_json() else {}
            apps_script_url = data.get('apps_script_url', '')
            
            # å¦‚æœæ²’æœ‰å¾è«‹æ±‚ä¸­æä¾› URLï¼Œå‰‡ä½¿ç”¨é…ç½®ä¸­çš„ URL
            if not apps_script_url:
                apps_script_url = Config.GOOGLE_APPS_SCRIPT_URL
                
            # å¦‚æœ URL ä»ç„¶æ˜¯ä½”ä½ç¬¦ï¼Œè¿”å›éŒ¯èª¤
            if apps_script_url == 'YOUR_ACTUAL_GOOGLE_APPS_SCRIPT_URL_HERE' or not apps_script_url:
                return jsonify({
                    'error': 'è«‹å…ˆåœ¨é…ç½®ä¸­è¨­å®š Google Apps Script URL æˆ–åœ¨è«‹æ±‚ä¸­æä¾›',
                    'message': 'è«‹åƒè€ƒ GOOGLE_APPS_SCRIPT_SETUP.md æ–‡ä»¶ä¾†éƒ¨ç½²æ‚¨çš„ Google Apps Scriptï¼Œç„¶å¾Œæ›´æ–°ç¨‹å¼ä¸­çš„ URL æˆ–é…ç½®æª”ã€‚'
                }), 400
            
            # æ”¶é›†è·ç¼ºè³‡æ–™ - ä½¿ç”¨èˆ‡ results å’Œ download ç›¸åŒçš„é‚è¼¯
            all_jobs = []
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºPDFæˆ–å¤šæª”æ¡ˆ
            pdf_page_keys = [key for key in image_storage._storage.keys() if key.startswith(f"{process_id}_page")]
            multi_file_keys = [key for key in image_storage._storage.keys() if key.startswith(f"{process_id}_file")]
            
            if pdf_page_keys:
                # PDFæƒ…æ³
                pdf_page_keys.sort(key=lambda x: int(x.split('_page')[-1]))
                
                for page_key in pdf_page_keys:
                    page_num = page_key.split('_page')[-1]
                    
                    for filename, image_data in image_storage._storage[page_key].items():
                        # è·³éåµéŒ¯åœ–åƒ
                        if any(debug_type in filename for debug_type in ['_original', '_mask_', '_final_combined']):
                            continue
                        
                        # ç›´æ¥å¾ image_storage ç²å–æè¿°
                        description = []
                        if page_key in image_storage._storage and \
                           filename in image_storage._storage[page_key] and \
                           'description' in image_storage._storage[page_key][filename]:
                            description = image_storage._storage[page_key][filename]['description']
                        
                        # æ”¶é›†æœ‰æ•ˆçš„å·¥ä½œè³‡è¨Šä¸¦æ·»åŠ ä¾†æºåœ–ç‰‡è³‡è¨Š
                        if isinstance(description, list):
                            for job in description:
                                if is_valid_job(job):
                                    job_info = job.copy()
                                    job_info['ä¾†æºåœ–ç‰‡'] = filename
                                    job_info['åœ–ç‰‡ç·¨è™Ÿ'] = f"page{page_num}_{filename.split('.')[0]}"
                                    all_jobs.append(job_info)
            elif multi_file_keys:
                # å¤šæª”æ¡ˆæƒ…æ³
                multi_file_keys.sort()

                for file_key in multi_file_keys:
                    # è§£ææª”æ¡ˆç·¨è™Ÿå’Œå¯èƒ½çš„é ç¢¼
                    key_parts = file_key.replace(f"{process_id}_", "").split('_')
                    if 'page' in file_key:
                        # å¤šæª”æ¡ˆPDFçš„æƒ…æ³
                        file_part = key_parts[0]  # file01
                        page_part = key_parts[1]  # page1
                        page_num = page_part.replace('page', '')
                        file_display = f"{file_part}_page{page_num}"
                    else:
                        # å¤šæª”æ¡ˆåœ–ç‰‡çš„æƒ…æ³
                        file_display = key_parts[0]
                        page_num = file_display

                    for filename, image_data in image_storage._storage[file_key].items():
                        # è·³éåµéŒ¯åœ–åƒ
                        if any(debug_type in filename for debug_type in ['_original', '_mask_', '_final_combined']):
                            continue
                        
                        # ç›´æ¥å¾ image_storage ç²å–æè¿°
                        description = []
                        if file_key in image_storage._storage and \
                           filename in image_storage._storage[file_key] and \
                           'description' in image_storage._storage[file_key][filename]:
                            description = image_storage._storage[file_key][filename]['description']

                        # æ”¶é›†æœ‰æ•ˆçš„å·¥ä½œè³‡è¨Šä¸¦æ·»åŠ ä¾†æºåœ–ç‰‡è³‡è¨Š
                        if isinstance(description, list):
                            for job in description:
                                if is_valid_job(job):
                                    job_info = job.copy()
                                    job_info['ä¾†æºåœ–ç‰‡'] = filename
                                    job_info['åœ–ç‰‡ç·¨è™Ÿ'] = f"{file_display}_{filename.split('.')[0]}"
                                    all_jobs.append(job_info)
            else:
                # å–®æª”æ¡ˆæƒ…æ³
                if process_id in image_storage._storage:
                    for filename, image_data in image_storage._storage[process_id].items():
                        # è·³éåµéŒ¯åœ–åƒ
                        if any(debug_type in filename for debug_type in ['_original', '_mask_', '_final_combined']):
                            continue
                        
                        # ç›´æ¥å¾ image_storage ç²å–æè¿°
                        description = []
                        if process_id in image_storage._storage and \
                           filename in image_storage._storage[process_id] and \
                           'description' in image_storage._storage[process_id][filename]:
                            description = image_storage._storage[process_id][filename]['description']

                        # æ”¶é›†æœ‰æ•ˆçš„å·¥ä½œè³‡è¨Šä¸¦æ·»åŠ ä¾†æºåœ–ç‰‡è³‡è¨Š
                        if isinstance(description, list):
                            for job in description:
                                if is_valid_job(job):
                                    job_info = job.copy()
                                    job_info['ä¾†æºåœ–ç‰‡'] = filename
                                    job_info['åœ–ç‰‡ç·¨è™Ÿ'] = filename.split('.')[0]
                                    all_jobs.append(job_info)
                else:
                    return jsonify({'error': 'è™•ç†çµæœä¸å­˜åœ¨'}), 404
            
            if not all_jobs:
                return jsonify({'error': 'æ²’æœ‰æœ‰æ•ˆçš„è·ç¼ºè³‡æ–™å¯ç™¼é€'}), 404
            
            # æº–å‚™ç™¼é€åˆ° Google Apps Script çš„è³‡æ–™
            payload = {
                'action': 'addJobs',
                'jobs': all_jobs,
                'metadata': {
                    'process_id': process_id,
                    'total_jobs': len(all_jobs),
                    'timestamp': datetime.now().isoformat(),
                    'source': 'newspaper_job_extractor'
                }
            }
            
            # ç™¼é€è«‹æ±‚åˆ° Google Apps Script
            response = requests.post(apps_script_url, json=payload, timeout=60)
            response.raise_for_status()
            
            # å˜—è©¦è§£æ Google Apps Script çš„å›æ‡‰
            try:
                result = response.json() if response.content else {}
            except:
                result = {'response_text': response.text}
            
            return jsonify({
                'success': True,
                'message': f'æˆåŠŸç™¼é€ {len(all_jobs)} ç­†è·ç¼ºè³‡æ–™åˆ° Google Sheets',
                'sent_jobs': len(all_jobs),
                'spreadsheet_url': result.get('spreadsheet_url', ''),
                'spreadsheet_id': result.get('spreadsheet_id', ''),
                'response': result
            }), 200
            
        except requests.exceptions.RequestException as e:
            return jsonify({'error': f'é€£æ¥ Google Apps Script å¤±æ•—: {str(e)}'}), 500
        except Exception as e:
            return jsonify({'error': f'ç™¼é€è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}'}), 500

    return app, socketio

def start_cleanup_scheduler():
    """å•Ÿå‹•å®šæ™‚æ¸…ç†ä»»å‹™"""
    def run_scheduled_cleanup():
        print("åŸ·è¡Œå®šæ™‚æ¸…ç†ä»»å‹™...")
        # å…ˆåŸ·è¡Œæ™‚é–“åŸºç¤çš„æ¸…ç†
        cleanup_old_files(Config.UPLOAD_FOLDER, Config.RESULTS_FOLDER, Config.CLEANUP_MAX_AGE_HOURS) 
        # å†åŸ·è¡Œæ•¸é‡é™åˆ¶æ¸…ç†ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        if Config.CLEANUP_ENABLE_COUNT_LIMIT:
            cleanup_service.cleanup_by_file_count()

    # è¨­ç½®æ¯4å°æ™‚åŸ·è¡Œä¸€æ¬¡æ¸…ç†
    schedule.every(Config.CLEANUP_INTERVAL_HOURS).hours.do(run_scheduled_cleanup)
    
    # åœ¨èƒŒæ™¯åŸ·è¡Œç·’ä¸­é‹è¡Œæ’ç¨‹å™¨
    def schedule_runner():
        while True:
            schedule.run_pending()
            time.sleep(60)  # æ¯åˆ†é˜æª¢æŸ¥ä¸€æ¬¡
    
    cleanup_thread = threading.Thread(target=schedule_runner, daemon=True)
    cleanup_thread.start()
    cleanup_status = "å·²å•Ÿç”¨" if Config.CLEANUP_ENABLE_COUNT_LIMIT else "æ™‚é–“æ¸…ç†å·²å•Ÿç”¨ï¼Œæ•¸é‡é™åˆ¶å·²åœç”¨"
    print(f"å®šæ™‚æ¸…ç†ä»»å‹™å·²å•Ÿå‹•ï¼Œæ¯{Config.CLEANUP_INTERVAL_HOURS}å°æ™‚åŸ·è¡Œä¸€æ¬¡ï¼ˆ{cleanup_status}ï¼‰")

# å‰µå»ºæ‡‰ç”¨å¯¦ä¾‹
app, socketio = create_app()

if __name__ == '__main__':
    # ç«‹å³åŸ·è¡Œä¸€æ¬¡æ¸…ç†ï¼ˆæ¸…ç†å•Ÿå‹•æ™‚çš„èˆŠæª”æ¡ˆï¼‰
    cleanup_old_files(Config.UPLOAD_FOLDER, Config.RESULTS_FOLDER, Config.CLEANUP_MAX_AGE_HOURS)

    # åŸ·è¡Œæ•¸é‡é™åˆ¶æ¸…ç†ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
    if Config.CLEANUP_ENABLE_COUNT_LIMIT:
        cleanup_service.cleanup_by_file_count()
    
    # å¾é…ç½®è®€å–ä¼ºæœå™¨è¨­å®š
    host = Config.FLASK_HOST
    port = int(os.environ.get('PORT', Config.FLASK_PORT))
    debug = Config.FLASK_DEBUG
    
    print(f"ğŸš€ å•Ÿå‹•å ±ç´™å·¥ä½œå»£å‘Šæå–ç³»çµ±...")
    print(f"ğŸ“¡ ä¼ºæœå™¨åœ°å€: http://{host}:{port}")
    print(f"ğŸ”§ é™¤éŒ¯æ¨¡å¼: {debug}")
    print(f"ğŸ§¹ è‡ªå‹•æ¸…ç†: æ¯{Config.CLEANUP_INTERVAL_HOURS}å°æ™‚åŸ·è¡Œ")
    cleanup_status = f"æœ€å¤šä¿ç•™ {Config.CLEANUP_MAX_FILE_COUNT} å€‹æª”æ¡ˆ" if Config.CLEANUP_ENABLE_COUNT_LIMIT else "å·²åœç”¨"
    print(f"ğŸ“ æª”æ¡ˆæ•¸é‡é™åˆ¶: {cleanup_status}")
    
    # å•Ÿå‹•æ¸…ç†èª¿åº¦å™¨
    try:
        start_cleanup_scheduler()
    except Exception as e:
        print(f"âš ï¸ æ¸…ç†èª¿åº¦å™¨å•Ÿå‹•è­¦å‘Š: {e}")
    
    socketio.run(app, debug=debug, host=host, port=port, allow_unsafe_werkzeug=True) 